{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\tf\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# lightFm imports\n",
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "from lightfm import cross_validation\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "# re for text cleaning\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"./careervillageDataset/\"\n",
    "\n",
    "df_answer_scores=pd.read_csv(base_path+'answer_scores.csv')\n",
    "df_answers=pd.read_csv(base_path+'answers.csv',parse_dates=['answers_date_added'])\n",
    "df_comments=pd.read_csv(base_path+'comments.csv')\n",
    "df_emails=pd.read_csv(base_path+'emails.csv')\n",
    "df_group_memberships=pd.read_csv(base_path+'group_memberships.csv')\n",
    "df_groups=pd.read_csv(base_path+'groups.csv')\n",
    "df_matches=pd.read_csv(base_path+'matches.csv')\n",
    "df_professionals=pd.read_csv(base_path+'professionals.csv',parse_dates=['professionals_date_joined'])\n",
    "df_question_scores=pd.read_csv(base_path+'question_scores.csv')\n",
    "df_questions=pd.read_csv(base_path+'questions.csv',parse_dates=['questions_date_added'])\n",
    "df_school_memberships=pd.read_csv(base_path+'school_memberships.csv')\n",
    "df_students=pd.read_csv(base_path+'students.csv',parse_dates=['students_date_joined'])\n",
    "df_tag_questions=pd.read_csv(base_path+'tag_questions.csv')\n",
    "df_tag_users=pd.read_csv(base_path+'tag_users.csv')\n",
    "df_tags=pd.read_csv(base_path+'tags.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_int_id(dataframe,id_col_name):\n",
    "    \"\"\"\n",
    "    Generate unique integer id for users, questions and answers\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        Pandas Dataframe for Users or Q&A. \n",
    "    id_col_name : String \n",
    "        New integer id's column name.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe\n",
    "        Updated dataframe containing new id column \n",
    "    \"\"\"\n",
    "    new_dataframe=dataframe.assign(\n",
    "        int_id_col_name=np.arange(len(dataframe))\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return new_dataframe.rename(columns={'int_id_col_name':id_col_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(dataframe, features_name, id_col_name):\n",
    "    \"\"\"\n",
    "    Generate features that will be ready for feeding into lightfm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        Pandas Dataframe which contains features\n",
    "    features_name : List\n",
    "        List of feature columns name avaiable in dataframe\n",
    "    id_col_name: String\n",
    "        Column name which contains id of the question or\n",
    "        answer that the features will map to.\n",
    "        There are two possible values for this variable.\n",
    "        1. questions_id_num\n",
    "        2. professionals_id_num\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas Series\n",
    "        A pandas series containing process features\n",
    "        that are ready for feed into lightfm.\n",
    "        The format of each value\n",
    "        will be (user_id, ['feature_1', 'feature_2', 'feature_3'])\n",
    "        Ex. -> (1, ['military', 'army', '5'])\n",
    "    \"\"\"\n",
    "    features=dataframe[features_name].apply(lambda x:','.join(x.map(str)),axis=1)\n",
    "    features=features.str.split(',')\n",
    "    features = list(zip(dataframe[id_col_name], features))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_list(dataframe, features_name):\n",
    "    \"\"\"\n",
    "    Generate features list for mapping \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        Pandas Dataframe for Users or Q&A. \n",
    "    features_name : List\n",
    "        List of feature columns name avaiable in dataframe. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of all features for mapping \n",
    "    \"\"\"\n",
    "    features = dataframe[features_name].apply(lambda x: ','.join(x.map(str)), axis=1)\n",
    "    features = features.str.split(',')\n",
    "    features = features.apply(pd.Series).stack().reset_index(drop=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_auc_score(lightfm_model, interactions_matrix, \n",
    "                        question_features, professional_features): \n",
    "    \"\"\"\n",
    "    Measure the ROC AUC metric for a model. \n",
    "    A perfect score is 1.0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lightfm_model: LightFM model \n",
    "        A fitted lightfm model \n",
    "    interactions_matrix : \n",
    "        A lightfm interactions matrix \n",
    "    question_features, professional_features: \n",
    "        Lightfm features \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    String containing AUC score \n",
    "    \"\"\"\n",
    "    score = auc_score( \n",
    "        lightfm_model, interactions_matrix, \n",
    "        item_features=question_features, \n",
    "        user_features=professional_features, \n",
    "        num_threads=4).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate numeric identifier: LightFM python only except numeric id. But the data we have has uuid for identifying users and professionals and others. In this step, I will make unique identifier for each professionals, students, questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating unique integer id for users and q&a\n",
    "df_professionals = generate_int_id(df_professionals, 'professionals_id_num')\n",
    "df_students = generate_int_id(df_students, 'students_id_num')\n",
    "df_questions = generate_int_id(df_questions, 'questions_id_num')\n",
    "df_answers = generate_int_id(df_answers, 'answers_id_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important steps for our solution. Our professionals, students, q&a and tags are stored in seperate datasets. For purpose of model, we have to merge our datasets in very carefull way so that they are useful for our model.\n",
    "\n",
    "1. All tags (q&a) are stored in a separate dataset. So firstly we merge those tags with questions and answers datasets.\n",
    "2. Then, we merge answers with quesitons because one question can have multiple answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = df_tags.dropna()\n",
    "df_tags['tags_tag_name'] = df_tags['tags_tag_name'].str.replace('#', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tag_questions with tags name\n",
    "# then group all tags for each question into single rows\n",
    "\n",
    "df_tags_question = df_tag_questions.merge(\n",
    "    df_tags,how='inner',left_on='tag_questions_tag_id',right_on='tags_tag_id'\n",
    ")\n",
    "\n",
    "df_tags_question = df_tags_question.groupby(\n",
    "    ['tag_questions_question_id'])['tags_tag_name'].apply(\n",
    "        ','.join).reset_index()\n",
    "\n",
    "df_tags_question = df_tags_question.rename(columns={'tags_tag_name': 'questions_tag_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tag_users with tags name \n",
    "# then group all tags for each user into single rows \n",
    "# after that rename the tag column name \n",
    "\n",
    "df_tags_pro=df_tag_users.merge(\n",
    "    df_tags,\n",
    "    how='inner',\n",
    "    left_on='tag_users_tag_id',\n",
    "    right_on='tags_tag_id'\n",
    ")\n",
    "\n",
    "df_tags_pro=df_tags_pro.groupby(\n",
    "    ['tag_users_user_id'])['tags_tag_name'].apply(','.join).reset_index()\n",
    "\n",
    "df_tags_pro = df_tags_pro.rename(columns={'tags_tag_name': 'professionals_tag_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge professionals and questions tags with main merge_dataset \n",
    "df_questions = df_questions.merge(\n",
    "    df_tags_question, how='left',\n",
    "    left_on='questions_id', right_on='tag_questions_question_id')\n",
    "df_professionals = df_professionals.merge(\n",
    "    df_tags_pro, how='left',\n",
    "    left_on='professionals_id', right_on='tag_users_user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge questions with scores \n",
    "df_questions = df_questions.merge(\n",
    "    df_question_scores, how='left',\n",
    "    left_on='questions_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge questions with students \n",
    "df_questions = df_questions.merge(\n",
    "    df_students, how='left',\n",
    "    left_on='questions_author_id', right_on='students_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge answers with questions \n",
    "# then merge professionals and questions score with that \n",
    "df_merge = df_answers.merge(\n",
    "    df_questions, how='inner',\n",
    "    left_on='answers_question_id', right_on='questions_id')\n",
    "df_merge = df_merge.merge(\n",
    "    df_professionals, how='inner',\n",
    "    left_on='answers_author_id', right_on='professionals_id')\n",
    "# df_merge = df_merge.merge(\n",
    "#     df_question_scores, how='inner',\n",
    "#     left_on='questions_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['answers_id', 'answers_author_id', 'answers_question_id',\n",
       "       'answers_date_added', 'answers_body', 'answers_id_num', 'questions_id',\n",
       "       'questions_author_id', 'questions_date_added', 'questions_title',\n",
       "       'questions_body', 'questions_id_num', 'tag_questions_question_id',\n",
       "       'questions_tag_name', 'id', 'score', 'students_id', 'students_location',\n",
       "       'students_date_joined', 'students_id_num', 'professionals_id',\n",
       "       'professionals_location', 'professionals_industry',\n",
       "       'professionals_headline', 'professionals_date_joined',\n",
       "       'professionals_id_num', 'tag_users_user_id', 'professionals_tag_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate some features:** In this steps, we are going to generate some features. We are going to generate number of answers by professionals, num of answers in each question, num of tags per professionals and number of tags per question. I will not use all of these features in this model. But I will use number of answers per question for weighting our model so that our model pay less attention to those quesitons that have higher number of answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['num_of_answers_by_professional']=df_merge.groupby(['answers_author_id'])['questions_id'].transform('count')\n",
    "df_merge['num_ans_per_ques']=df_merge.groupby(['questions_id'])['answers_id'].transform('count')\n",
    "df_merge['num_tags_professional'] = df_merge['professionals_tag_name'].str.split(\",\").str.len()\n",
    "df_merge['num_tags_question'] = df_merge['questions_tag_name'].str.split(\",\").str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of answer per question : 58\n",
      "Maximum number of tags per professional : 82.0\n",
      "Maximum number of tags per question : 54.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum number of answer per question : \" + str(df_merge['num_ans_per_ques'].max()))\n",
    "print(\"Maximum number of tags per professional : \" + str(df_merge['num_tags_professional'].max()))\n",
    "print(\"Maximum number of tags per question : \" + str(df_merge['num_tags_question'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge answered questions tags with professional's tags:** Professionals can follow some tags. But not all professional follow tags and most especially we see from EDA that sometime professionals answers questions that is not related to their tags. For that reason, I have merge questions tags that each professional has answered with professional tags. This makes our model more robust and context aware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select professionals answered questions tags \n",
    "# and stored as a dataframe\n",
    "professionals_prev_ans_tags = df_merge[['professionals_id', 'questions_tag_name']]\n",
    "\n",
    "# drop null values from that \n",
    "professionals_prev_ans_tags = professionals_prev_ans_tags.dropna()\n",
    "\n",
    "# we group all of tags of each user into single row \n",
    "professionals_prev_ans_tags = professionals_prev_ans_tags.groupby(\n",
    "    ['professionals_id'])['questions_tag_name'].apply(\n",
    "        ','.join).reset_index()\n",
    "\n",
    "# drop duplicates tags from each professionals rows\n",
    "professionals_prev_ans_tags['questions_tag_name'] = (\n",
    "    professionals_prev_ans_tags['questions_tag_name'].str.split(',').apply(set).str.join(','))\n",
    "\n",
    "# finally merge the dataframe with professionals dataframe \n",
    "df_professionals = df_professionals.merge(professionals_prev_ans_tags, how='left', on='professionals_id')\n",
    "\n",
    "# join professionals tags and their answered tags \n",
    "# we replace nan values with \"\"\n",
    "df_professionals['professional_all_tags'] = (\n",
    "    df_professionals[['professionals_tag_name', 'questions_tag_name']].apply(\n",
    "        lambda x: ','.join(x.dropna()),\n",
    "        axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Handling null and duplicates values:** Now we want clean our data a little bit. We will handle null and duplicate values. Because if we don't remove that they will cause error and wrong prediction. Also, we will replace null values with generic name or value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling null values \n",
    "df_questions['score'] = df_questions['score'].fillna(0)\n",
    "df_questions['score'] = df_questions['score'].astype(int)\n",
    "df_questions['questions_tag_name'] = df_questions['questions_tag_name'].fillna('No Tag')\n",
    "# remove duplicates tags from each questions \n",
    "df_questions['questions_tag_name'] = df_questions['questions_tag_name'].str.split(',').apply(set).str.join(',')\n",
    "\n",
    "\n",
    "# fill nan with 'No Tag' if any \n",
    "df_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].fillna('No Tag')\n",
    "# replace \"\" with \"No Tag\", because previously we replace nan with \"\"\n",
    "df_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].replace('', 'No Tag')\n",
    "df_professionals['professionals_location'] = df_professionals['professionals_location'].fillna('No Location')\n",
    "df_professionals['professionals_industry'] = df_professionals['professionals_industry'].fillna('No Industry')\n",
    "\n",
    "# remove duplicates tags from each professionals \n",
    "df_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].str.split(',').apply(set).str.join(',')\n",
    "\n",
    "\n",
    "\n",
    "# remove some null values from df_merge\n",
    "df_merge['num_ans_per_ques']  = df_merge['num_ans_per_ques'].fillna(0)\n",
    "df_merge['num_tags_professional'] = df_merge['num_tags_professional'].fillna(0)\n",
    "df_merge['num_tags_question'] = df_merge['num_tags_question'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model in LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightFM Datset class makes it really easy for us for creating interection matrix, weights and user/item features.\n",
    "\n",
    "- interection matrix: It is a matrix that contains user/ item interections or professional/quesiton intereactions.\n",
    "- weights: weight of interection matrix. Less weight means less importance to that interection matrix.\n",
    "- user/item features: user/item features supplied as like this (user_id, ['feature_1', 'feature_2', 'feature_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating features list for Dataset class: LightFM library has a Dataset class that makes it really easy for building necessary information for model. But we have feed set of all professionals/questions unique ids and all questions and professional features list. This will create internel mapping for lightFM to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating features list for mapping \n",
    "question_feature_list = generate_feature_list(\n",
    "    df_questions,\n",
    "    ['questions_tag_name'])\n",
    "\n",
    "professional_feature_list = generate_feature_list(\n",
    "    df_professionals,\n",
    "    ['professional_all_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate our weight value \n",
    "df_merge['total_weights'] = 1 / (\n",
    "    df_merge['num_ans_per_ques'])\n",
    "\n",
    "\n",
    "# creating features for feeding into lightfm \n",
    "df_questions['question_features'] = create_features(\n",
    "    df_questions, ['questions_tag_name'], \n",
    "    'questions_id_num')\n",
    "\n",
    "\n",
    "df_professionals['professional_features'] = create_features(\n",
    "    df_professionals,\n",
    "    ['professional_all_tags'],\n",
    "    'professionals_id_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightFM Dataset:** building lightfm datasets. Building our interactions matrix, weights and professional/question features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our dataset variable\n",
    "# then we feed unique professionals and questions ids\n",
    "# and item and professional feature list\n",
    "# this will create lightfm internel mapping\n",
    "dataset=Dataset()\n",
    "dataset.fit(\n",
    "    set(df_professionals['professionals_id_num']),\n",
    "    set(df_questions['questions_id_num']),\n",
    "    item_features=question_feature_list,\n",
    "    user_features=professional_feature_list)\n",
    "\n",
    "# now we are building interactions matrix between professionals and quesitons\n",
    "# we are passing professional and questions id as a tuple\n",
    "# e.g -> pd.Series((pro_id, question_id), (pro_id, questin_id))\n",
    "# then we use lightfm build in method for building interactions matrix\n",
    "df_merge['author_question_id_tuple'] = list(zip(\n",
    "    df_merge.professionals_id_num, df_merge.questions_id_num, df_merge.total_weights))\n",
    "\n",
    "\n",
    "interactions, weights = dataset.build_interactions(\n",
    "    df_merge['author_question_id_tuple'])\n",
    "\n",
    "# now we are building our questions and professionals features\n",
    "# in a way that lightfm understand.\n",
    "# we are using lightfm build in method for building\n",
    "# questions and professionals features \n",
    "questions_features=dataset.build_item_features(\n",
    "    df_questions['question_features']\n",
    ")\n",
    "professional_features=dataset.build_user_features(\n",
    "    df_professionals['professional_features']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lightfm model by specifying hyper-parametre\n",
    "# then fit the model with ineteractions matrix, item and user features \n",
    "model=LightFM(\n",
    "    no_components=150,\n",
    "    learning_rate=0.05,\n",
    "    loss='bpr',\n",
    "    random_state=2019\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    interactions,\n",
    "    item_features=questions_features,\n",
    "    user_features=professional_features,\n",
    "    sample_weight=weights,\n",
    "    epochs=5,\n",
    "    num_threads=4,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\tf\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from lightfm import LightFM\n",
    "from lightfm.datasets import fetch_movielens\n",
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "# Load the MovieLens 100k dataset\n",
    "data = fetch_movielens()\n",
    "\n",
    "# Create and train the model\n",
    "model = LightFM(loss='warp')\n",
    "model.fit(data['train'], epochs=10, num_threads=1)\n",
    "\n",
    "# Evaluate the model\n",
    "train_precision = precision_at_k(model, data['train'], k=10).mean()\n",
    "test_precision = precision_at_k(model, data['test'], k=10).mean()\n",
    "\n",
    "print(f'Train precision: {train_precision}')\n",
    "print(f'Test precision: {test_precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
